Multicollinearity
------------------

- When two or more independent variables in multiple regression model are highly correlated. (0.8 or 0.9)

ex : cgpa iq lpa 

- cgpa and iq are highly correlated


interpretation of coef : 

lpa = B0 + B1*cgpa + B2*iq


lets talk about coef:
-------------------

B1- if you keep iq constant , By increasing cgpa by 1 unit , the factor at which lpa is increased is B1

B2 - if you keep cgpa constant.and increase iq by 1 unit , by how much factor the lpa will increase is B2 


==========================================================================================================================================================

Problem with multicollinearity
-------------------------------

lpa = B0 + B1*cgpa + B2*iq

now , if their is multicollinearity , the interpretation of coef can be wrong.

ex : if you increase cgpa by 1 unit , then iq wont remain constant as they are highly correlated. and if iq wont remain constant , then you cant interpret the value of B2. ie B2 value isnt reliable


conclusion : IN case of multicollinearity the coefs and intercept become unreliable.

==========================================================================================================================================================


When is multicollinearity bad?
------------------------------

before undestanding this you must know the concept of prediction and inference

There are two works of machine learning : prediction and inference

Prediction  :
-------------  
- focus on using a model to make forcasts for unseen data.


Inference   :
---------
- understanding the relationship between variables in model . 
- how much the input columns , output column affect each other.

 In banking scenario , inference is more important. 

ex : giving loan to customer. We will use not for predictions. Instead we will study the features based on which loan will given or not will be decided.



conclusion :
------------

Not bad - if you are using model for prediction , then multicollinearity would not affect

bad : when you are using for inference. ie studying the relation of input output columns. as inference totally depend on coef. And if there is multicollinearity , then the coef will not be reliable . Hence we cannot inference


========================================================================================================================================================

What  exactly happens in multicollinearity (Maths)?

1. Difficulty in identifying most importatnt predictors:

Multicollinearity makes coef unreliable , thats why we dont know which has the significant effect on dependent variable.


2. Inflated Standard errors

SE(B) taks high value


3. Unstable and unreliable estimaes : coef




To understand maths , we must first understand perfect multicollinearity



Perfect Multicollinearity
--------------------------

In ideal scenario we have 2 input and 1 output columns iq , cgpa and lpa

If x1 and x2 is highly correlated. then we can express one column in the form of linear regression

 x1 = a1*x2 + a0 + error(irreducible error)



But in case of perfect multicollinearity , there is no error, therefore the eqt will be 

x1 = a1*x2 + a0 -> perfect multicollinearity.



Example of perfect multicollinearity
-------------------------------------

cgpa percent lpa
8.5  85      7
9.12  91.2   6


percent  = 10 *cgpa + 0   (No error involved)

Now our goal is to fit linear regression and find coef on this perfect multicollinearity data.


lpa = b0 + b1*cgpa + b2*percent + error

we know that B = (B^T B)^-1(refer pdf)

if there is a perfect multicollinearity, the determinant becomes zero , hence B^T B becomes singular matrix , and we cant find the inverse. Hence B values cant be calculated/

Conclusion : In the perfect multicollinearity scenario , you cant find the values of B ie b1 and b2. 

But generally perfect mutlicollinearity cant be their in real life.



Non perfect Multicollinearity
------------------------------

In high or non perfect mutlicollinearity ,To prove two things :

1. Unstable coefficients
2. High SE

Formula for SE(B) = root(diag( sigma^2 (X^T X)^-1))

refer to pdf to see formula


High SE mathematical proof:
----------------------------

- if there is high multicollinearity , that means the determinant becomes very small (as in perfect multicoll the determinant becomes 0). 

- if determinant is very small , and in inverse we divide by 1/deter and if det is very small , then all the values inside matrix(X transpose X)^-1 will inflate. 

- and in standard error formula we multiplied by constant , the diognal elements/standard error will increase. 


problem - if standard error is high , then there will be uncertainty.ie area for to be population regression line will be very large.


Unstable estimates
-------------------

- not mathematical but refer jupyter notebook to see the practical 

problem - coef(b1 , b2 , b0) becomes very sensitive to change in input data. ie means little change in input data , will lead to big change in coef values.



Now if you see the section of individual study of columns , you will see the whole section depend on B and standard error and if these are unreliable , then the t test , p values becomes unreliable. Hence Mutlicollinearity affects purpose of inference , but not for prediction 





===========================================================================================================================================================


Types of multicollinearity
--------------------------


1. Structural Multicollinearity

- yourself introduces multicollinearity by feature engineering. Ex One hot encoding , polynomial features

one hot encoding can lead to dummy variable trap . 


2. Data driven multicollinearity 

- when data you have is highly correlated 

ex : sqft and no of bathrooms are highly correlated leading to multicollinearity





How to detect Multicollinearity ?
-------------------------------

1. Correlation 
2. VIF - variance inflation factor
3. Condition no -> regression analysis. genereally , If condition no > 30 , then you have strong multicollinearity


1. Correlation :
------------------

if you can write x1 = a1*x2 + a0 + error , then you can say x1 and x2 strong correlation (linear dependence) . hence multicollinearity

2. Variance Inflation factor
------------------------------
- if you have doubt , you can perform linear regression by making 1 input col ->output. and others as input. 
and same goes for other columns.

- if you have 3 input col , you run 3 linear regression and find r2 score each time for each linear regression and from r2 score you calculate vif = 1/(1 - r2score)

- if you have vif score greater than 5 or 10 for a column , then there is a mutlicollinearity between 3 columns.

- and if vif score is near 1 or 1 then there may not be multicollinearity

refer to jupyter notebook  

3. Condition number
--------------------