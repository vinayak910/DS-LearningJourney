- When curse of dimensionality we use feature selection or feature extraction
- Generally feature selection is done after model building . 


Curse of dimensionality
------------------------

- Curse of features
- There are optimum no of features are required, as you start increasing the no of features of that optimum point , there will no effect on increasing accuracy and model performance. Instead the performance and will degrade in some chances.


When you will encounter high dimensional data
---------------------------------------------

1. Images
2. Text data




What actually happens in higher dimesion ?
----------------------------------------------
- ans is sparsity

- example : 
- you lost your wallet, tell in which case finding your wallet is easy. 
- 1. college gate to main gate. 
- 2. Playground
- 3. College building.

- The easiest case to find you wallet is college gate to main gate , as it is 1d space/direction. When playground->2d space , when building -> 3d space. 

- the wallet is same , but as you go into higher dimension , it becomes harder to find the wallet, because of sparse. 

- IN higher dimension, data is getting sparse(data is getting away from each other. 

- therefore in higher dimension , the ml models wont perform well . 
- EX knn considers nearest neighbor for prediction, but in higher dimension . The data will get sparse and they wont be neighbors anymore . hence KNN will not performs well on high dimensional data.



2 Problems with high dimensional data
--------------------------------------

1. Performance decreases
2. Computation 



How to solve ?
--------------

- dimensionality reduction. 
- there are 2 ways for dimensionality reduction 

1. Feature selection 
2. Feature extraction : PCA , LDA , TSNE