recap : 
--------

- there are some algo which have high bias and some have high variance

Bias variance tradeoff
-----------------------

low bias -> high variance
low variance -> high bias


True function vs our function
------------------------------

y = f(x) + error
y_hat = f'(x)


bias(f'(x)) => E[f'(x) - f(x)]

var(f'(x))  => E[(f'(x) - E[f'(x)])^2]


===================================================================================================================================================================



Continued
----------



Bias Variance Decomposition
----------------------------

- ex : IN linear regression , we have a loss function : mse ie (y - y_hat)^2/n.
- So what bias variance decomposition do is it divides the loss function into 3 parts ->

Loss fn = Bias + variance + irreducible error

exact eq of loss fn = bias^2 + variance + Var(E)  
- here in Var(E),E is epsilon not E 

- var(E) indicates variance of irreducible error

- here , bias^2 + variance is reducible error and var(E) is irreducible error



derivation of loss fn = bias^2 + variance + Var(E)
---------------------------------------------------- 

refer to register


Non mathematical intution 
--------------------------

refer to pdf 


Question 
----------

Why their is bias square in Bias variance decomposition ?
- due to our loss function , since our loss function is mse . Due to this , our bias is squared.
- if our loss function changed , the bias may also changed.




Conclusion : 
-------------

- You are data scientist , you want to make a perfect model , but you are making multiple imperfect models.

- and gap between perfect and imperfect model is error/loss.

- our error is of three components

1. Bias : ON and avg how far we are from prediction of actual values

2. Variance : on and avg how much our prediction is far away from our predictions(as there are multiple prediction lines , the predition distance from each sample regression lines is variance)

3. Noise : Noise in the data. 


In supervise machine learning , this will always be valid. 



Post credits 
-------------

Ml models and introduction of regularization


control bias : Bias can be controlled /low by complex machine learning models such as high polynomial lr , neural networks , decision tree which will capture the training data , but at the same time .as we are using complex ml models to reduce bias ,  it will increase the variance. as there is a trade off. 


technique to control variance : and Now to control the variance , we use variance techinques , and one of the famous techinque is regularization , which will decrease the variance in the imperfect ml models . use to reduce overfitting.