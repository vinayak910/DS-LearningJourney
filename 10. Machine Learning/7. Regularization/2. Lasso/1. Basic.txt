Lasso
------
Just like ridge , lasso is also a regularization technique to reduce overfitting.

- the penalty term added to mse loss function is lamda*mod of weights/coef.
- that's why it is called L1 norm , as there is only power of 1. 
The big difference is :
- IN lasso the slope/weights can easily become 0 . ie feature whose slope become zero indicates , there is no contribution of that feature in perdiction of y.



Benefit of becoming slopes for features
------------------------------------------
 
- In a high dimensional data , there is high chance of overfitting.
- if you apply ridge regression the slopes of any feature will not be zero
- But if you apply lasso regression , for higher value of lambda , the features that are less important , their slope will become zero. ie you are performing feature selection. 

- that's why you should use lasso in high dimension , as you are reducing overfitting and also doing feature selection
refer to jupyter notebook to see practical implementation of L1 



4 key points (refer to jupyter notebook)
----------------

1. How coef are affected?

- if you start increasing lambda, the coef starts to shrink down hence or even become zero (ultimately performing feature selection) , the model may underfit. 
- if lambda = 0 , then it is simple linear regression. 


2. Coeff with higher values are affected more. 

3. Impact on Bias and variance

- if you start increasing lambda variance will decrease, overfitting will reduce ,but the bias will increase

4. impact on loss function

- the slope value can cross zero



lasso summary 
---------------
Lasso regression, also known as L1 regularization or Lasso (Least Absolute Shrinkage and Selection Operator), is a linear regression technique that adds a regularization term to the loss function to prevent overfitting and improve the model's ability to generalize to unseen data.

The loss function for Lasso regression is given by:

L(w) = 1/2n \* Σ(y\_i - (w^T x\_i))^2 + α \* ||w||1

where w represents the model coefficients or weights, x\_i represents the input features, y\_i represents the target variable, n represents the number of training examples, and α represents the regularization parameter.

The L1 regularization term, α \* ||w||1, adds a penalty proportional to the absolute value of the coefficient values. This encourages smaller coefficient values, reducing overfitting and improving generalization.

The L1 regularization term has the effect of shrinking some of the coefficient values to zero, effectively eliminating them from the model. This can be useful for feature selection, as it allows us to identify the most important features for predicting the target variable.

Lasso regression is particularly useful when the number of input features is larger than the number of training examples, as it can help prevent overfitting and improve the model's ability to generalize to unseen data.

The regularization parameter, α, controls the strength of the regularization term. A larger value of α results in stronger regularization and smaller coefficient values, while a smaller value of α results in weaker regularization and larger coefficient values.

Lasso regression can be trained using various optimization algorithms, such as coordinate descent or proximal gradient descent. Once trained, the model can be used to make predictions on new, unseen data.

Lasso regression is widely used in various applications, such as predicting housing prices, stock prices, sales revenue, and many others. However, it is important to note that Lasso regression assumes a linear relationship between the input features and the target variable, and may not perform well if this assumption is violated. In such cases, non-linear regression models, such as polynomial regression or spline regression, may be more appropriate.