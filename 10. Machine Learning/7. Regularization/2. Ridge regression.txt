regularization in linear regression are of 2 types 

1. Ridge regularization (L2)
2. Lasso Regularization  (L1)


L2 / Ridge regularization
--------------------------

- We have to choose a line that doesnt overfit. ie for a training data it is giving good result , but for test data it is giving best result.

- we want our model to not to be overfit, which we achieve by adding a something to the mse loss function. ANd that something in L2 is Lambda * slope^2
- we use square of slopes , thats why it is called L2 norm. 

- Now the model will find the minima on the new loss function ie mse + lambda*slope^2 , which will prevent our line to be generalized.


Summary of Ridge
----------------
Ridge regression, also known as Tikhonov regularization or L2 regularization, is a technique used in machine learning and statistics to address issues of overfitting in regression models. Overfitting occurs when a model learns the training data too well, including the noise, and performs poorly on unseen data.

Ridge regression introduces a penalty term to the loss function, which is the sum of squares of the model coefficients. This penalty term discourages large coefficient values, leading to a simpler, more generalized model. The strength of the penalty is controlled by a hyperparameter, often denoted as λ or alpha. A larger λ value results in smaller coefficient values and a higher bias but lower variance, while a smaller λ value allows for larger coefficients, increasing the variance but reducing the bias.

The ridge regression estimator can be calculated using a closed-form solution or through iterative methods like gradient descent. The closed-form solution for the ridge regression coefficients (β) can be calculated as:

β = (X^T X + λI)^-1 X^T y

where X is the design matrix, y is the response vector, I is the identity matrix, and λ is the regularization parameter.

To choose the optimal λ value, techniques such as cross-validation, grid search, or automated methods like Bayesian optimization can be employed.

In summary, ridge regularization is a helpful technique for improving the performance and generalizability of regression models by reducing overfitting and controlling the complexity of the model.

